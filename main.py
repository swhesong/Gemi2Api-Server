import asyncio
import json
from datetime import datetime, timezone
import os
import base64
import re
import tempfile
from pathlib import Path
from fastapi import FastAPI, HTTPException, Request, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any, Union
import time
import uuid
import logging

# æ·»åŠ  dotenv æ”¯æŒ
from dotenv import load_dotenv

from gemini_webapi import GeminiClient, set_log_level, logger
from gemini_webapi.constants import Model
from gemini_webapi.exceptions import AuthError, APIError, TimeoutError

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
set_log_level("INFO")

app = FastAPI(title="Gemini API FastAPI Server", version="0.2.0")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global client with better management
gemini_client = None
client_last_used = None
CLIENT_IDLE_TIMEOUT = 300  # 5 minutes idle timeout

# Authentication credentials - æ”¯æŒå¤šç§ç¯å¢ƒå˜é‡æ ¼å¼
SECURE_1PSID = os.environ.get("SECURE_1PSID") or os.environ.get("CONFIG_GEMINI__CLIENTS__0__SECURE_1PSID", "")
SECURE_1PSIDTS = os.environ.get("SECURE_1PSIDTS") or os.environ.get("CONFIG_GEMINI__CLIENTS__0__SECURE_1PSIDTS", "")
API_KEY = os.environ.get("API_KEY") or os.environ.get("CONFIG_SERVER__API_KEY", "")

# Proxy support
GEMINI_PROXY = os.environ.get("GEMINI_PROXY", "")

# Client configuration - ä½¿ç”¨å®Œæ•´çš„é…ç½®é€‰é¡¹
CLIENT_CONFIG = {
    "timeout": 300,
    "auto_close": True,
    "close_delay": CLIENT_IDLE_TIMEOUT,
    "auto_refresh": True,
    "refresh_interval": 540,  # 9 minutes
}

# å¦‚æœæœ‰ä»£ç†é…ç½®ï¼Œæ·»åŠ åˆ°å®¢æˆ·ç«¯é…ç½®ä¸­
if GEMINI_PROXY:
    CLIENT_CONFIG["proxy"] = GEMINI_PROXY

# å¯åŠ¨æ—¶çš„é…ç½®æ£€æŸ¥å’Œä¼˜åŒ–
@app.on_event("startup")
async def startup_event():
    logger.info("ğŸš€ Starting Gemini API FastAPI Server v0.2.0")
    
    if not SECURE_1PSID:
        if not SECURE_1PSIDTS:
            logger.warning("âš ï¸ No Gemini credentials provided. Will attempt to use browser cookies if available.")
        else:
            logger.warning("âš ï¸ Only SECURE_1PSIDTS provided. SECURE_1PSID is required.")
    else:
        logger.info(f"âœ… Credentials found. SECURE_1PSID starts with: {SECURE_1PSID[:5]}...")
        if SECURE_1PSIDTS:
            logger.info(f"âœ… SECURE_1PSIDTS starts with: {SECURE_1PSIDTS[:5]}...")

    if not API_KEY:
        logger.warning("âš ï¸ API_KEY is not set or empty! API authentication will not work.")
        logger.warning("Make sure API_KEY is correctly set in your .env file or environment.")
    else:
        logger.info(f"âœ… API_KEY found. API_KEY starts with: {API_KEY[:5]}...")

    if GEMINI_PROXY:
        logger.info(f"ğŸŒ Proxy configured: {GEMINI_PROXY}")

@app.on_event("shutdown")
async def shutdown_event():
    global gemini_client
    if gemini_client:
        try:
            await gemini_client.close()
            logger.info("ğŸ‘‹ Gemini client closed on shutdown")
        except Exception as e:
            logger.warning(f"Error during client shutdown: {e}")


def correct_markdown(md_text: str) -> str:
    """
    ä¿®æ­£Markdownæ–‡æœ¬ï¼Œç§»é™¤Googleæœç´¢é“¾æ¥åŒ…è£…å™¨ï¼Œå¹¶æ ¹æ®æ˜¾ç¤ºæ–‡æœ¬ç®€åŒ–ç›®æ ‡URLã€‚
    """
    def simplify_link_target(text_content: str) -> str:
        match_colon_num = re.match(r"([^:]+:\d+)", text_content)
        if match_colon_num:
            return match_colon_num.group(1)
        return text_content

    def replacer(match: re.Match) -> str:
        outer_open_paren = match.group(1)
        display_text = match.group(2)

        new_target_url = simplify_link_target(display_text)
        new_link_segment = f"[`{display_text}`]({new_target_url})"

        if outer_open_paren:
            return f"{outer_open_paren}{new_link_segment})"
        else:
            return new_link_segment
    
    pattern = r"(\()?\[`([^`]+?)`\]\((https://www.google.com/search\?q=)(.*?)(?<!\\)\)\)*(\))?"
    
    fixed_google_links = re.sub(pattern, replacer, md_text)
    # fix wrapped markdownlink
    pattern = r"`(\[[^\]]+\]\([^\)]+\))`"
    return re.sub(pattern, r'\1', fixed_google_links)


# Pydantic models for API requests and responses
class ContentItem(BaseModel):
    type: str
    text: Optional[str] = None
    image_url: Optional[Dict[str, str]] = None


class Message(BaseModel):
    role: str
    content: Union[str, List[ContentItem]]
    name: Optional[str] = None


class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 1.0
    n: Optional[int] = 1
    stream: Optional[bool] = False
    max_tokens: Optional[int] = None
    presence_penalty: Optional[float] = 0
    frequency_penalty: Optional[float] = 0
    user: Optional[str] = None


class Choice(BaseModel):
    index: int
    message: Message
    finish_reason: str


class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Choice]
    usage: Usage


class ModelData(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "google"


class ModelList(BaseModel):
    object: str = "list"
    data: List[ModelData]


# Authentication dependency
async def verify_api_key(authorization: str = Header(None)):
    if not API_KEY:
        # If API_KEY is not set in environment, skip validation (for development)
        logger.warning("API key validation skipped - no API_KEY set in environment")
        return

    if not authorization:
        raise HTTPException(status_code=401, detail="Missing Authorization header")
    
    try:
        scheme, token = authorization.split()
        if scheme.lower() != "bearer":
            raise HTTPException(status_code=401, detail="Invalid authentication scheme. Use Bearer token")
        
        if token != API_KEY:
            raise HTTPException(status_code=401, detail="Invalid API key")
    except ValueError:
        raise HTTPException(status_code=401, detail="Invalid authorization format. Use 'Bearer YOUR_API_KEY'")
    
    return token


# Simple error handler middleware
@app.middleware("http")
async def error_handling(request: Request, call_next):
    try:
        return await call_next(request)
    except Exception as e:
        logger.error(f"Request failed: {str(e)}")
        return JSONResponse(status_code=500, content={"error": {"message": str(e), "type": "internal_server_error"}})


# Health check endpoint for Docker
@app.get("/health")
async def health_check():
    """Health check endpoint for container orchestration"""
    return {"status": "healthy", "timestamp": datetime.now(tz=timezone.utc).isoformat()}


# Root endpoint
@app.get("/")
async def root():
    return {
        "status": "online", 
        "message": "Gemini API FastAPI Server is running",
        "version": "0.2.0",
        "health": "/health"
    }


# Get list of available models
@app.get("/v1/models")
async def list_models():
    """è¿”å› gemini_webapi ä¸­å£°æ˜çš„æ¨¡å‹åˆ—è¡¨"""
    now = int(datetime.now(tz=timezone.utc).timestamp())
    data = [
        {
            "id": m.model_name,  # å¦‚ "gemini-2.5-flash"
            "object": "model",
            "created": now,
            "owned_by": "google-gemini-web",
        }
        for m in Model
    ]
    logger.info(f"Available models: {[d['id'] for d in data]}")
    return {"object": "list", "data": data}


# Helper to convert between Gemini and OpenAI model names
def map_model_name(openai_model_name: str) -> Model:
    """æ ¹æ®æ¨¡å‹åç§°å­—ç¬¦ä¸²æŸ¥æ‰¾åŒ¹é…çš„ Model æšä¸¾å€¼"""
    # æ‰“å°æ‰€æœ‰å¯ç”¨æ¨¡å‹ä»¥ä¾¿è°ƒè¯•
    all_models = [m.model_name if hasattr(m, "model_name") else str(m) for m in Model]
    logger.info(f"Available models: {all_models}")

    # é¦–å…ˆå°è¯•ç›´æ¥æŸ¥æ‰¾åŒ¹é…çš„æ¨¡å‹åç§°
    for m in Model:
        model_name = m.model_name if hasattr(m, "model_name") else str(m)
        if openai_model_name.lower() in model_name.lower():
            return m

    # å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…é¡¹ï¼Œä½¿ç”¨æ–°çš„æ˜ å°„è§„åˆ™ï¼ˆåŸºäºæœ€æ–°çš„æ¨¡å‹ï¼‰
    model_keywords = {
        # æ–°çš„æ¨¡å‹æ˜ å°„
        "gemini-2.5-flash": ["2.5", "flash"],
        "gemini-2.5-pro": ["2.5", "pro"],
        # å…¼å®¹æ—§çš„æ˜ å°„
        "gemini-2.0-flash": ["2.0", "flash"],
        "gemini-2.0-flash-thinking": ["2.0", "thinking"],
        "gemini-pro": ["pro"],
        "gemini-pro-vision": ["vision", "pro"],
        "gemini-1.5-pro": ["1.5", "pro"],
        "gemini-1.5-flash": ["1.5", "flash"],
    }

    # æ ¹æ®å…³é”®è¯åŒ¹é…
    keywords = model_keywords.get(openai_model_name, ["flash"])  # é»˜è®¤ä½¿ç”¨flashæ¨¡å‹

    for m in Model:
        model_name = m.model_name if hasattr(m, "model_name") else str(m)
        if all(kw.lower() in model_name.lower() for kw in keywords):
            return m

    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œè¿”å›é»˜è®¤æ¨¡å‹
    try:
        return Model.G_2_5_FLASH  # ä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
    except AttributeError:
        return next(iter(Model))


# Prepare conversation history from OpenAI messages format
def prepare_conversation(messages: List[Message]) -> tuple:
    conversation = ""
    temp_files = []

    for msg in messages:
        if isinstance(msg.content, str):
            # String content handling
            if msg.role == "system":
                conversation += f"System: {msg.content}\n\n"
            elif msg.role == "user":
                conversation += f"Human: {msg.content}\n\n"
            elif msg.role == "assistant":
                conversation += f"Assistant: {msg.content}\n\n"
        else:
            # Mixed content handling
            if msg.role == "user":
                conversation += "Human: "
            elif msg.role == "system":
                conversation += "System: "
            elif msg.role == "assistant":
                conversation += "Assistant: "

            for item in msg.content:
                if item.type == "text":
                    conversation += item.text or ""
                elif item.type == "image_url" and item.image_url:
                    # Handle image - ä½¿ç”¨ tempfile.NamedTemporaryFile æ›´ç¨³å®šçš„æ–¹å¼
                    image_url = item.image_url.get("url", "")
                    if image_url.startswith("data:image/"):
                        # Process base64 encoded image
                        try:
                            # Extract the base64 part
                            base64_data = image_url.split(",")[1]
                            image_data = base64.b64decode(base64_data)

                            # Create temporary file to hold the image
                            with tempfile.NamedTemporaryFile(delete=False, suffix=".png") as tmp:
                                tmp.write(image_data)
                                temp_files.append(tmp.name)
                        except Exception as e:
                            logger.error(f"Error processing base64 image: {str(e)}")

            conversation += "\n\n"

    # Add a final prompt for the assistant to respond to
    conversation += "Assistant: "

    return conversation, temp_files


async def get_gemini_client():
    global gemini_client, client_last_used
    
    current_time = time.time()
    
    # Check if client needs reinitialization
    if (gemini_client is None or 
        not hasattr(gemini_client, 'running') or
        not gemini_client.running or
        (client_last_used and current_time - client_last_used > CLIENT_IDLE_TIMEOUT)):
        
        # Clean up existing client
        if gemini_client:
            try:
                await gemini_client.close()
            except Exception as e:
                logger.warning(f"Error closing existing client: {str(e)}")
        
        try:
            # Initialize with better error handling and auto-refresh
            if SECURE_1PSID or SECURE_1PSIDTS:
                gemini_client = GeminiClient(
                    secure_1psid=SECURE_1PSID or None,
                    secure_1psidts=SECURE_1PSIDTS or None
                )
            else:
                # å°è¯•ä½¿ç”¨æµè§ˆå™¨cookie
                gemini_client = GeminiClient()
                
            await gemini_client.init(**CLIENT_CONFIG)
            logger.info("âœ… Gemini client initialized successfully with enhanced features")
        except AuthError as e:
            logger.error(f"âŒ Authentication failed: {str(e)}")
            raise HTTPException(
                status_code=401, 
                detail="Authentication failed. Please check your cookies."
            )
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Gemini client: {str(e)}")
            raise HTTPException(
                status_code=500, 
                detail=f"Failed to initialize Gemini client: {str(e)}"
            )
    
    client_last_used = current_time
    return gemini_client


@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest, api_key: str = Depends(verify_api_key)):
    temp_files = []
    try:
        # ä½¿ç”¨æ”¹è¿›çš„å®¢æˆ·ç«¯è·å–å‡½æ•°
        gemini_client_local = await get_gemini_client()
        
        # è½¬æ¢æ¶ˆæ¯ä¸ºå¯¹è¯æ ¼å¼
        conversation, temp_files = prepare_conversation(request.messages)
        logger.info(f"ğŸ“ Prepared conversation: {conversation[:200]}...")
        logger.info(f"ğŸ–¼ï¸ Temp files: {temp_files}")
        
        # è·å–é€‚å½“çš„æ¨¡å‹
        model = map_model_name(request.model)
        logger.info(f"ğŸ¤– Using model: {model}")
        
        # ç”Ÿæˆå“åº”ï¼Œä½¿ç”¨é‡è¯•æœºåˆ¶
        logger.info("ğŸš€ Sending request to Gemini...")
        max_retries = 3
        response = None
        
        for attempt in range(max_retries):
            try:
                if temp_files:
                    # With files
                    response = await gemini_client_local.generate_content(
                        conversation, files=temp_files, model=model
                    )
                else:
                    # Text only
                    response = await gemini_client_local.generate_content(conversation, model=model)
                break
            except (AuthError, APIError, TimeoutError) as e:
                logger.warning(f"âš ï¸ Request attempt {attempt + 1} failed: {str(e)}")
                if attempt == max_retries - 1:
                    # å¯¹äºç‰¹å®šçš„å¼‚å¸¸ç±»å‹ï¼ŒæŠ›å‡ºç›¸åº”çš„HTTPå¼‚å¸¸
                    if isinstance(e, AuthError):
                        raise HTTPException(status_code=401, detail="Authentication failed")
                    elif isinstance(e, TimeoutError):
                        raise HTTPException(status_code=408, detail=f"Request timeout: {str(e)}")
                    else:
                        raise HTTPException(status_code=502, detail=f"API error: {str(e)}")
                # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯
                global gemini_client
                gemini_client = None
                gemini_client_local = await get_gemini_client()
                await asyncio.sleep(1)  # çŸ­æš‚å»¶è¿Ÿ

        if response is None:
            raise HTTPException(status_code=500, detail="Failed to get response after retries")

        # å¤„ç†å“åº”å†…å®¹ - ä½¿ç”¨æ¸…æ™°çš„é€»è¾‘
        reply_text = ""
        
        # æå–æ€è€ƒå†…å®¹
        if hasattr(response, "thoughts") and response.thoughts:
            reply_text += f"<think>{response.thoughts}</think>\n\n"
        
        # å¤„ç†ä¸»è¦å“åº”å†…å®¹
        if hasattr(response, "text"):
            reply_text += response.text
        else:
            reply_text += str(response)
        
        # å­—ç¬¦è½¬ä¹‰å¤„ç†
        reply_text = reply_text.replace("&lt;", "<").replace("\\<", "<").replace("\\_", "_").replace("\\>", ">")
        
        # åº”ç”¨è‡ªå®šä¹‰çš„markdownä¿®æ­£
        reply_text = correct_markdown(reply_text)
        
        logger.info(f"ğŸ’¬ Response generated: {len(reply_text)} characters")

        if not reply_text or reply_text.strip() == "":
            logger.warning("âš ï¸ Empty response received from Gemini")
            reply_text = "æœåŠ¡å™¨è¿”å›äº†ç©ºå“åº”ã€‚è¯·æ£€æŸ¥ Gemini API å‡­æ®æ˜¯å¦æœ‰æ•ˆã€‚"

        # åˆ›å»ºå“åº”å¯¹è±¡
        completion_id = f"chatcmpl-{uuid.uuid4()}"
        created_time = int(time.time())

        # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦è¯·æ±‚æµå¼å“åº”
        if request.stream:
            # å®ç°æµå¼å“åº”
            async def generate_stream():
                # åˆ›å»º SSE æ ¼å¼çš„æµå¼å“åº”
                # å…ˆå‘é€å¼€å§‹äº‹ä»¶
                data = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}],
                }
                yield f"data: {json.dumps(data)}\n\n"

                # æ¨¡æ‹Ÿæµå¼è¾“å‡º - å°†æ–‡æœ¬æŒ‰å­—ç¬¦åˆ†å‰²å‘é€
                for char in reply_text:
                    data = {
                        "id": completion_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": request.model,
                        "choices": [{"index": 0, "delta": {"content": char}, "finish_reason": None}],
                    }
                    yield f"data: {json.dumps(data)}\n\n"
                    # å¯é€‰ï¼šæ·»åŠ çŸ­æš‚å»¶è¿Ÿä»¥æ¨¡æ‹ŸçœŸå®çš„æµå¼è¾“å‡º
                    await asyncio.sleep(0.01)

                # å‘é€ç»“æŸäº‹ä»¶
                data = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": request.model,
                    "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
                }
                yield f"data: {json.dumps(data)}\n\n"
                yield "data: [DONE]\n\n"

            return StreamingResponse(generate_stream(), media_type="text/event-stream")
        else:
            # éæµå¼å“åº”ï¼ˆåŸæ¥çš„é€»è¾‘ï¼‰
            result = {
                "id": completion_id,
                "object": "chat.completion",
                "created": created_time,
                "model": request.model,
                "choices": [{"index": 0, "message": {"role": "assistant", "content": reply_text}, "finish_reason": "stop"}],
                "usage": {
                    "prompt_tokens": len(conversation.split()),
                    "completion_tokens": len(reply_text.split()),
                    "total_tokens": len(conversation.split()) + len(reply_text.split()),
                },
            }

            logger.info(f"âœ… Returning response with {result['usage']['total_tokens']} tokens")
            return result
            
    except HTTPException:
        # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
        raise
    except Exception as e:
        logger.error(f"âŒ Unexpected error generating completion: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")
    finally:
        # ä¸´æ—¶æ–‡ä»¶æ¸…ç†
        cleaned_files = []
        failed_cleanups = []
        
        for temp_file in temp_files:
            try:
                os.unlink(temp_file)
                cleaned_files.append(temp_file)
            except Exception as e:
                failed_cleanups.append((temp_file, str(e)))
                logger.warning(f"âš ï¸ Failed to delete temp file {temp_file}: {str(e)}")
        
        # è®°å½•æ¸…ç†ç»“æœ
        if cleaned_files:
            logger.info(f"ğŸ§¹ Successfully cleaned up {len(cleaned_files)} temporary files")
            
        if failed_cleanups:
            logger.warning(f"âš ï¸ Failed to clean up {len(failed_cleanups)} temporary files")


# Entry point for running with uvicorn
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, log_level="info")
